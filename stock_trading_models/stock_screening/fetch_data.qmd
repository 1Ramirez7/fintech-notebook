---
title: "python_test"
format: html
---



**What I need** 

I need a list of all US equity stocks. Code juse uses US equity so I either have a list with just symbols or remove US EQUITY with code before running.

```{python}
# this code gets daily closing price for a list of stocks and set time frame. 
# tag: closing list
# code 1

import yfinance as yf
import pandas as pd

# Load Excel file
file_path = r"data/ticker_sample.xlsx"
df = pd.read_excel(file_path)

# Define time frame
start_date = "2024-05-13"
end_date = "2025-06-07"

# Get daily closing prices with each date in its own column
def get_closing_prices(ticker):
    try:
        data = yf.Ticker(ticker).history(start=start_date, end=end_date)
        closes = data["Close"]
        closes.index = closes.index.strftime("%Y-%m-%d")
        return closes
    except:
        return pd.Series()

# Apply and expand into separate columns
closes_df = df["ticker"].apply(get_closing_prices).apply(pd.Series)
closes_df.columns.name = None

# Combine and save
result = pd.concat([df, closes_df], axis=1)
result.to_excel(r"data/ticker_sample.xlsx", index=False)

```



```{python}
# this code is intended for one value. For some reason it was a pain trying to just get one day so I just use the same as above and updated to make sure the date is the last price available.
# End date needs to be one day after target value. even if I want todays last price, I need to use tomorrows as my end_date. 
# tag: closing last-price

import yfinance as yf
import pandas as pd

# Load Excel file
file_path = r"data/ticker_sample.xlsx"
df = pd.read_excel(file_path)

# Define time frame
start_date = "2024-06-10" # target date here
end_date = "2025-06-11" # must be one day ahead of target date. 


def get_closing_prices(ticker):
    try:
        data = yf.Ticker(ticker).history(start=start_date, end=end_date)
        closes = data["Close"]
        closes.index = closes.index.strftime("%Y-%m-%d")
        return closes
    except:
        return pd.Series()


closes_df = df["ticker"].apply(get_closing_prices).apply(pd.Series)
closes_df.columns.name = None

# Combine and save
result = pd.concat([df, closes_df], axis=1)
result.to_excel(r"data/close_price.xlsx", index=False)


```






**Code gets closing price for list of stocks - this code works with csv.**
- Cell 4568
- It reads and edits same file
- so tickers are as variables/column-names
- dates are as row values 


```{python}
# cell 4568
import yfinance as yf
import pandas as pd

# Load existing CSV with Date as index
file_path = r"data/close_price.csv"
df = pd.read_csv(file_path, parse_dates=["Date"])
df.set_index("Date", inplace=True)

# Define time frame
start_date = "2024-04-01"
end_date = "2024-05-12"

# Get list of tickers from column names
tickers = df.columns.tolist()

# Fetch new closing prices
def get_closing_prices(ticker):
    try:
        data = yf.Ticker(ticker).history(start=start_date, end=end_date)
        closes = data["Close"].rename(ticker)
        return closes
    except:
        return pd.Series(name=ticker)

# Combine all new prices
new_data = pd.concat([get_closing_prices(t) for t in tickers], axis=1)

# Combine old + new, keeping the latest if duplicate dates
combined = pd.concat([df, new_data])
combined = combined[~combined.index.duplicated(keep='last')]
combined.sort_index(inplace=True)

# Save back to CSV
combined.reset_index().to_csv(file_path, index=False)

# Ok this code reads the ticker names from the column titles, and has the date values in the rows, so each row is for one unique date. 

```







**Making dim_date table**

- Ok I'm making a cs file that has calendar_date starting from 2000-01-01 and will also make the date_id off the celendar_date. 
- This is so I already have set date_id for each date. I think it will make things easier maybe. 

```{python}
# code 2
# makes dim_Date table
import pandas as pd

# Set manual end date
end_date = "2027-12-31"  # ← change this as needed

# Create date range from 2000-01-01 to end_date
dates = pd.date_range(start="2000-01-01", end=end_date)
# Create DataFrame with date_id and calendar_date
df = pd.DataFrame({
    "calendar_date": dates,
    "date_id": dates.strftime("%Y%m%d").astype(int)
})

# Optional: sort by date_id just to confirm uniqueness
df = df[["date_id", "calendar_date"]]

# Save to CSV
df.to_csv("date_dim.csv", index=False)


```



**Making dim_ticker table** 

- will have ticker_id, symbol, company_name, sector, industry

I initally wanted to generated ticker_id using alphabet order and a spacer like 77 between letters but it was not properly working with 5 characters and also I needed to handle the data type as csv was converting the ticker_id to scientific notation (only for larger numbers). 
- Data warehousing: least commmon dinomanator.


```{python}

import pandas as pd

# Load your CSV
file_path = "sql/index.csv"  # change to your actual file path
df = pd.read_csv(file_path)

# Assign sequential ticker_id starting from 1
df['ticker_id'] = range(1, len(df) + 1)

# Save the updated file (overwrite)
df.to_csv(file_path, index=False)


```


**Making fact_close_price table**

- This table will have ticker_id, date_id, cloe_price, volume for now
- The primary_key will be a composite of ticker_id and date_id. 

so the primary_key will be like a surrogate key. Since my ticker_id starts from 1 and goes to 3200 or so. I have 10227 days I want to include in my data base. I want to make the surrogate key work off this order. for ticker_id 1, the first 10227 surrogate keys (start from 1) will be for ticker_id 1 which each surrogate key represents one unique date, the order is key so the ticker_id 2 will have surrogate key form 10227 to 20454 and so on. 

The python chunk below (cell 73747) succesfully makes the id column which is the primary key or composite key of ticker_id and date_id or surrogate key for the fact table. 

```{python}
# cell 73747
# code 0
import pandas as pd
from datetime import datetime

# Load CSV
file_path = "sql/fact_close_price.csv"
df = pd.read_csv(file_path)

# Convert date_id to (yyyy/mm/dd)
df["calendar_date"] = pd.to_datetime(df["date_id"].astype(str), format="%Y%m%d").dt.strftime("%Y/%m/%d")
base_date = datetime.strptime("2000/01/01", "%Y/%m/%d") # Set base_date

# Convert calendar_date back to datetime for calculation
df["id"] = pd.to_datetime(df["calendar_date"], format="%Y/%m/%d").apply(lambda x: (x - base_date).days + 1)

# Adjust id based on ticker_id
df["id"] = df.apply(lambda row: row["id"] if row["ticker_id"] == 1 else ((row["ticker_id"] - 1) * 10227 + row["id"]), axis=1)
df.drop(columns=["calendar_date"], inplace=True)


# save/edit csv
df.to_csv("sql/fact_close_price.csv", index=False)

#print(df.head())


```



**Continuation of making the fact_close_price table**
-deleted-
**This code is continuation of making fact_close_price table**
-deleted-

**Code to pull stock information**

Ok the code does the same as above, but I'm now trying to pull the ticker list from index.csv and joined df_new to the main csv file. 
- This is so new data does not overwrite old data as it is doing now. 
- This code will add by "id" and only update target columns in update_cols when id exist in main csv. 
- if id is new to main csv, then it will add all columns in new_rows


```{python}
# === Build / update fact_close_price.csv ===
# 1) loads tickers & ticker_id from existing fact_close_price.csv
# 2) fetches close prices in the date-range you set
# 3) converts to date_id & builds surrogate id
# 4) appends non-duplicate rows back to fact_close_price.csv
# -----------------------------------------------------------

# 1.	Reads fact_close_price.csv to get ticker and ticker_id (code 0).
# 2.	Uses yfinance to get closing prices for each ticker in a given date range (code 1).
# 3.	Converts those dates to date_id using the format from the dim_Date table (code 2).
# 4.	Expands the ticker_id to match all available date_ids from the price data.
# 5.	Creates a unique id based on the formula from code 0:
#    1)	
#      ⦁	if ticker_id == 1, then id = day_count
#      ⦁	else, id = ((ticker_id - 1) * 10227) + day_count
# 1.	Returns a final DataFrame with columns: id, ticker_id, ticker, date_id, close_price.
# 2.	appends rows in fact_close_price.csv
import pandas as pd
from datetime import datetime
import yfinance as yf
import os
import time

# ── PARAMETERS ─────────────────────────────────────────────
FILE_PATH = "sql/fact_close_price.csv"  # existing fact table
START_DATE = "2025-06-14"                # inclusive
END_DATE = "2025-06-17"                  # exclusive (yfinance)
REQUEST_BATCH_SIZE = 1000                 # #'s' of tickers to process before a pause
PAUSE_DURATION_SECONDS = 10              # Duration of the pause
# -----------------------------------------------------------

# 1. Load existing fact table (for ticker list & to avoid duplicates)
if not os.path.exists(FILE_PATH):
    raise FileNotFoundError(f"{FILE_PATH} not found.")

df_existing = pd.read_csv(FILE_PATH)
ticker_lookup = pd.read_csv("sql/index.csv")[["ticker", "ticker_id"]].drop_duplicates()

# 2. Pull close prices and additional info for each ticker
def fetch_data(ticker):
    try:
        yf_ticker = yf.Ticker(ticker)
        
        # Fetch historical prices
        data = yf_ticker.history(start=START_DATE, end=END_DATE)
        if data.empty:
            print(f"No historical price data found for ticker: {ticker}")
            return pd.DataFrame()
        
        data = (data[["Close", "High", "Low", "Volume"]]
                .rename(columns={"Close": "close_price", "High": "dayHigh", "Low": "dayLow", "Volume": "volume"})
                .reset_index()
                .rename(columns={"Date": "date"})
        )
        data["date"] = data["date"].dt.date
        return data
    except Exception as e:
        print(f"Error fetching data for ticker {ticker}: {e}")
        return pd.DataFrame()

frames = []
for i, row in enumerate(ticker_lookup.itertuples(), start=1):
    data = fetch_data(row.ticker)
    if not data.empty: # Only append if data was successfully fetched
        data["ticker"] = row.ticker
        data["ticker_id"] = row.ticker_id
        frames.append(data)

    if i % REQUEST_BATCH_SIZE == 0:
        print(f"Processed {i} tickers... pausing for {PAUSE_DURATION_SECONDS} seconds.")
        time.sleep(PAUSE_DURATION_SECONDS)

if not frames:
    print("No new data retrieved in the specified range.")
    quit()

df_new = pd.concat(frames, ignore_index=True)

# 3. Add date_id (YYYYMMDD int)
df_new["date_id"] = pd.to_datetime(df_new["date"]).dt.strftime("%Y%m%d").astype(int)

# 4. Build surrogate id
BASE_DATE = datetime(2000, 1, 1)
TOTAL_DAYS = (datetime(2027, 12, 31) - BASE_DATE).days + 1  # 10227
df_new["day_count"] = (pd.to_datetime(df_new["date"]) - BASE_DATE).dt.days + 1
df_new["id"] = df_new.apply(
    lambda r: r["day_count"]
    if r["ticker_id"] == 1
    else ((r["ticker_id"] - 1) * TOTAL_DAYS + r["day_count"]),
    axis=1,
)

# 5. Select/rename final columns
df_new = df_new[["id", "ticker_id", "ticker", "date_id", "date", "close_price", "dayHigh", "dayLow", "volume"]]

# 6. Define columns to update in fact_close_price.csv
update_cols = ["close_price", "dayHigh", "dayLow", "volume"] # maybe include date_id & date?

df_updates = df_new[["id"] + update_cols].set_index("id")
df_existing.set_index("id", inplace=True)
df_existing.update(df_updates)

new_rows = df_new[~df_new["id"].isin(df_existing.index)].set_index("id")

df_final = pd.concat([df_existing, new_rows])
# 7. ─────────────────────────────────────────────────────────
df_final.reset_index(inplace=True)
df_final.sort_values(["ticker_id", "date_id"], inplace=True)

# Save back to CSV
df_final.to_csv(FILE_PATH, index=False)


print("Fact close price table updated successfully!")

```




**Closer to final fetch prices code**

This code is almost done
- fetches prices using index.csv and add prices to fact_close_price.csv (github and drive links)
- It handles when adding new id rows and when updating existing id rows. (improvements can be made)
- I will also make the eidts in google drive to avoid having to push to github each update to the csv files. 
- This code ONLY WORKS ON GOOGLE COLAB. 

```{python}
import pandas as pd
from datetime import datetime
import yfinance as yf
import os
import time
from google.colab import drive # mount drive
drive.mount('/content/drive') # mount drive


# ── PARAMETERS ─────────────────────────────────────────────
FILE_PATH = "/content/drive/My Drive/stock_data/fact_close_price.csv"  # existing fact table
START_DATE = "2024-04-14"                # inclusive
END_DATE = "2024-04-17"                  # exclusive (yfinance)
REQUEST_BATCH_SIZE = 120                 # #'s' of tickers to process before a pause
PAUSE_DURATION_SECONDS = 10              # Duration of the pause
# -----------------------------------------------------------

# 1. Load existing fact table (for ticker list & to avoid duplicates)
if not os.path.exists(FILE_PATH):
    raise FileNotFoundError(f"{FILE_PATH} not found.")

df_existing = pd.read_csv(FILE_PATH)
ticker_lookup = pd.read_csv("https://raw.githubusercontent.com/1Ramirez7/fintech-notebook/refs/heads/main/stock_trading_models/stock_screening/sql/index.csv")[["ticker", "ticker_id"]].drop_duplicates()

# 2. Pull close prices and additional info for each ticker
def fetch_data(ticker):
    try:
        yf_ticker = yf.Ticker(ticker)
        
        # Fetch historical prices
        data = yf_ticker.history(start=START_DATE, end=END_DATE)
        if data.empty:
            print(f"No historical price data found for ticker: {ticker}")
            return pd.DataFrame()
        
        data = (data[["Close", "High", "Low", "Volume"]]
                .rename(columns={"Close": "close_price", "High": "dayHigh", "Low": "dayLow", "Volume": "volume"})
                .reset_index()
                .rename(columns={"Date": "date"})
        )
        data["date"] = data["date"].dt.date
        return data
    except Exception as e:
        print(f"Error fetching data for ticker {ticker}: {e}")
        return pd.DataFrame()

frames = []
for i, row in enumerate(ticker_lookup.itertuples(), start=1):
    data = fetch_data(row.ticker)
    if not data.empty: # Only append if data was successfully fetched
        data["ticker"] = row.ticker
        data["ticker_id"] = row.ticker_id
        frames.append(data)

    if i % REQUEST_BATCH_SIZE == 0:
        print(f"Processed {i} tickers... pausing for {PAUSE_DURATION_SECONDS} seconds.")
        time.sleep(PAUSE_DURATION_SECONDS)

if not frames:
    print("No new data retrieved in the specified range.")
    quit()

df_new = pd.concat(frames, ignore_index=True)

# 3. Add date_id (YYYYMMDD int)
df_new["date_id"] = pd.to_datetime(df_new["date"]).dt.strftime("%Y%m%d").astype(int)

# 4. Build surrogate id
BASE_DATE = datetime(2000, 1, 1)
TOTAL_DAYS = (datetime(2027, 12, 31) - BASE_DATE).days + 1  # 10227
df_new["day_count"] = (pd.to_datetime(df_new["date"]) - BASE_DATE).dt.days + 1
df_new["id"] = df_new.apply(
    lambda r: r["day_count"]
    if r["ticker_id"] == 1
    else ((r["ticker_id"] - 1) * TOTAL_DAYS + r["day_count"]),
    axis=1,
)

# 5. Select/rename final columns
df_new = df_new[["id", "ticker_id", "ticker", "date_id", "date", "close_price", "dayHigh", "dayLow", "volume"]]

# 6. Define columns to update in fact_close_price.csv
update_cols = ["close_price", "dayHigh", "dayLow", "volume"] # maybe include date_id & date?

df_updates = df_new[["id"] + update_cols].set_index("id")
df_existing.set_index("id", inplace=True)
df_existing.update(df_updates)

new_rows = df_new[~df_new["id"].isin(df_existing.index)].set_index("id")

df_final = pd.concat([df_existing, new_rows])
# 7. ─────────────────────────────────────────────────────────
df_final.reset_index(inplace=True)
df_final.sort_values(["ticker_id", "date_id"], inplace=True)

# Save back to CSV
df_final.to_csv(FILE_PATH, index=False)


print("Fact close price table updated successfully!")


```






**Making dim_sector**

- I do not have a special primary_key but I should have a established way to get the primary key
- The goal is so the primary_key will generate the same if new sectors are added or re arrange
- So far this code just start from 1 base on order

```{python}
# this code just makes the sector_dim.csv but code below updates a target csv with the sector_id
import pandas as pd
# Load your CSV
file_path = "sql/index_main.csv"
df = pd.read_csv(file_path)

# Get unique sector values and assign IDs
sectors_df = (
    pd.DataFrame(df["sector"].dropna().unique(), columns=["sector"])
    .sort_values(by="sector")
    .reset_index(drop=True)
)
sectors_df["sector_id"] = sectors_df.index + 1  # Start IDs at 1

# Reorder columns
sectors_df = sectors_df[["sector_id", "sector"]]

# Save to new CSV
sectors_df.to_csv("sql/sector_dim.csv", index=False)

```


**Makes sector_dim table and updates target csv with the sector_id column where sector = sector.**

```{python}
import pandas as pd

# Path for the source CSV to extract unique sectors
file_path = "sql/index_main.csv"
df = pd.read_csv(file_path)

# --- Create Sector Dimension Table (sector_dim.csv) ---
sectors_df = (
    pd.DataFrame(df["sector"].dropna().unique(), columns=["sector"])
    .sort_values(by="sector")
    .reset_index(drop=True)
)
sectors_df["sector_id"] = sectors_df.index + 1
sectors_df = sectors_df[["sector_id", "sector"]]

sectors_df.to_csv("sql/sector_dim.csv", index=False)

# --- Update the Target CSV with sector_id --- It can be any table so long as it has sector and sector_id
df_target_path = "sql/index_main.csv"
df_target = pd.read_csv(df_target_path)

# Perform a left merge to add sector_id to df_target based on matching 'sector'
df_target = pd.merge(
    df_target, sectors_df, on="sector", how="left", suffixes=('', '_new_id')
)

# Handle existing 'sector_id' column in df_target
if 'sector_id_new_id' in df_target.columns and 'sector_id' in df_target.columns:
    df_target['sector_id'] = df_target['sector_id_new_id'].fillna(df_target['sector_id'])
    df_target = df_target.drop(columns=['sector_id_new_id'])
elif 'sector_id_new_id' in df_target.columns:
    df_target = df_target.rename(columns={'sector_id_new_id': 'sector_id'})

df_target.to_csv(df_target_path, index=False)
```

**Makes industry_dim table**

Same as code that makes the sector_dim table.

```{python}
# same code as the code that makes the sector_dim code. 
import pandas as pd
file_path = "sql/index_main.csv"
df = pd.read_csv(file_path)
industry_df = (
    pd.DataFrame(df["industry"].dropna().unique(), columns=["industry"])
    .sort_values(by="industry")
    .reset_index(drop=True)
)
industry_df["industry_id"] = industry_df.index + 1
industry_df = industry_df[["industry_id", "industry"]]
industry_df.to_csv("sql/industry_dim.csv", index=False)
df_target_path = "sql/index_main.csv"
df_target = pd.read_csv(df_target_path)
df_target = pd.merge(
    df_target, industry_df, on="industry", how="left", suffixes=('', '_new_id')
)
if 'industry_id_new_id' in df_target.columns and 'industry_id' in df_target.columns:
    df_target['industry_id'] = df_target['industry_id_new_id'].fillna(df_target['industry_id'])
    df_target = df_target.drop(columns=['industry_id_new_id'])
elif 'industry_id_new_id' in df_target.columns:
    df_target = df_target.rename(columns={'industry_id_new_id': 'industry_id'})

# Save the updated df_target back to its original file.
df_target.to_csv(df_target_path, index=False)
```


# Information so far

So right now I'm using the index.csv or index_main.csv as my raw list of stocks. Since its only unique stock values I'm able to have duplicates of values for example sector & indsutry. 
- but for my fact_close_price I will be better off if I use the ticker_id to fetch sector and indsutry and join as it will take up less memory. ticker_id is in index.csv, fact_close_price.csv, sector.csv and industry.csv. 





















































































































































































































































spacer