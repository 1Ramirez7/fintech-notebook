---
title: "r_code"
format: html
---


information


```{r}
# Load libraries
if (!require("pacman")) install.packages("pacman")
pacman::p_load(googledrive, readr, dplyr)

# Authenticate Google Drive
if (!drive_has_token()) drive_auth()

# Load and download the file
file <- drive_get(path = "stock_data/fact_close_price.csv")
temp_path <- tempfile(fileext = ".csv")
drive_download(file, path = temp_path, overwrite = TRUE)

# Read only first few rows to infer types
df <- read_csv(temp_path, n_max = 100)

# Display column names and data types
glimpse(df)


```




**Plot stock time series price**

```{r}

if (!require("pacman")) install.packages("pacman")
pacman::p_load(googledrive, readr, dplyr, ggplot2, lubridate)

# ── PARAMETERS ─────────────────────────────────────────────
ticker <- "CARM"
date_start <- as.Date("2025-06-10")
date_end <- as.Date("2025-06-21")
# ───────────────────────────────────────────────────────────

# load google drive file
if (!googledrive::drive_has_token()) {googledrive::drive_auth()} 
file <- drive_get(path = "stock_data/fact_close_price.csv")

# Download to temporary path
temp_path <- tempfile(fileext = ".csv")
drive_download(file, path = temp_path, overwrite = TRUE)

# Chunked read and filter
filtered_data <- list()

chunk_callback <- function(x, pos) {
  x <- x |>
    filter(ticker == !!ticker)

  if (nrow(x) > 0) {
    filtered_data[[length(filtered_data) + 1]] <<- x
  }
}

read_csv_chunked(
  file = temp_path,
  callback = SideEffectChunkCallback$new(chunk_callback),
  chunk_size = 100000,
  col_types = cols_only(
    ticker = col_character(),
    date = col_character(),
    close_price = col_double()
  )
)

# Combine filtered chunks
df <- bind_rows(filtered_data) |>
  mutate(date = as.Date(date)) |>
  filter(date >= date_start & date <= date_end)

# Plot
ggplot(df, aes(x = date, y = close_price)) +
  geom_line() +
  labs(
    title = paste0(ticker, " Close Price: ", date_start, " to ", date_end),
    x = "Date", y = "Close Price"
  ) +
  theme_minimal()

```


This code returns the top and bottom percent change difference

- from fact_close_price - Includes: ticker_id, ticker, date, close_price, volume
- from index_main - Includes: ticker_id, sector, industry
- joins sector and industry to the pivot_df using ticker_id



```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(googledrive, readr, dplyr, lubridate, tidyr)

# ── PARAMETERS ─────────────────────────────────────────────
previous_date <- as.Date("2025-06-17")
current_date  <- as.Date("2025-06-18")
# ───────────────────────────────────────────────────────────

# Authenticate
if (!googledrive::drive_has_token()) drive_auth()

# Load Drive file
file <- drive_get(path = "stock_data/fact_close_price.csv")
temp_path <- tempfile(fileext = ".csv")
drive_download(file, path = temp_path, overwrite = TRUE)

# select columns to read from fact_close_price (1 of 2)
df <- read_csv(
  file = temp_path,
  col_types = cols_only(
    ticker_id = col_double(),
    ticker = col_character(),
    date = col_character(),
    close_price = col_double(),
    volume = col_double()
  )
) |>
  mutate(date = as.Date(date)) |>
  filter(date %in% c(previous_date, current_date)) |>
  select(ticker_id, ticker, date, close_price, volume) # select columns to include from fact_close_price (2 of 2)

# Pivot and calculate % change
pivot_df <- df |>
  pivot_wider(
    names_from = date,
    values_from = c(close_price, volume),
    names_sep = "_"
  ) |>
  rename_with(
    ~ c("price_prev", "price_curr", "volume_prev", "volume_curr"),
    .cols = c(
      paste0("close_price_", previous_date),
      paste0("close_price_", current_date),
      paste0("volume_", previous_date),
      paste0("volume_", current_date)
    )
  ) |>
  filter(!is.na(price_prev) & !is.na(price_curr)) |>
  mutate(
    percent_change = 100 * (price_curr - price_prev) / price_prev
  ) |>
  arrange(desc(percent_change))

# Add sector from index_main.csv
index_df <- read_csv("https://raw.githubusercontent.com/1Ramirez7/fintech-notebook/refs/heads/main/stock_trading_models/stock_screening/sql/index_main.csv") |>
  select(ticker_id, sector, industry) # select columns to join from index_main to pivot_df (sector, idustry, etc)
pivot_df <- pivot_df |> # join to pivo_df
  left_join(index_df, by = "ticker_id") # make sure to add variable to result select.

# Results
top_10 <- head(pivot_df, 10)
bottom_10 <- tail(pivot_df, 10)

# Combine for output
result <- bind_rows(
  top_10 |> mutate(rank_group = "Top 10"),
  bottom_10 |> mutate(rank_group = "Bottom 10")
) |>
  select(rank_group, ticker, price_prev, price_curr, percent_change, volume_prev, volume_curr, sector, industry)

rm(df, pivot_df, index_df); gc()

print(result)

# I just thought of doing a time series plot for volume to see if i notice random spikes before large spikes in price

```






















































































































































































































































































































































































































































































































































































































































spacer





































